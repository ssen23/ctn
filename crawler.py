import requests
from bs4 import BeautifulSoup
from pymongo import MongoClient
from datetime import datetime
import time
import random

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC


# MongoDB ì—°ê²°
client = MongoClient("mongodb+srv://PASSWORD")
db = client["news_politics"]
collection = db["2025.06.09"]

# Selenium ë“œë¼ì´ë²„ ì„¸íŒ… (í—¤ë“œë¦¬ìŠ¤ ì˜µì…˜ í¬í•¨)
def get_selenium_driver():
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    driver = webdriver.Chrome(options=options)
    return driver


def crawl_politics_news():
    print("ğŸ” ì •ì¹˜ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘")
    
    headers = {'User-Agent': 'Mozilla/5.0'}

    # Selenium ë“œë¼ì´ë²„ ì‹œì‘
    driver = get_selenium_driver()
    driver.get("https://news.naver.com/section/100")
    wait = WebDriverWait(driver, 10)

    # â¬‡ï¸ 'ë”ë³´ê¸°' ë²„íŠ¼ ì—¬ëŸ¬ ë²ˆ í´ë¦­í•´ì„œ ê¸°ì‚¬ ë” ë§ì´ ë¡œë“œ
    MAX_CLICKS = 5 
    for i in range(MAX_CLICKS):
        try:
            more_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "a.section_more_inner._CONTENT_LIST_LOAD_MORE_BUTTON")))
            more_button.click()
            print(f"ğŸŸ¢ ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­: {i+1}íšŒ")
            time.sleep(2.5)
        except Exception:
            print("âŒ ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨ ë˜ëŠ” ë” ì´ìƒ ì—†ìŒ")
            break

    # ğŸ“„ í˜ì´ì§€ ë¡œë”© ì™„ë£Œ í›„ soup ìƒì„±
    html = driver.page_source
    soup = BeautifulSoup(html, "html.parser")
    articles = soup.select('ul.sa_list > li.sa_item')
    
    # ë§¨ ìœ„ 10ê°œ ê±´ë„ˆë›°ê¸° / í—¤ë“œë¼ì¸ ì œê±°
    articles = articles[10:]

    print(f"âœ… ì´ ê¸°ì‚¬ ìˆ˜ (ë”ë³´ê¸° í´ë¦­ í›„): {len(articles)}")

    count_saved = 0

    # Selenium ë“œë¼ì´ë²„ ì‹œì‘
    driver = get_selenium_driver()

    try:
        for idx, article in enumerate(articles):
            if count_saved >= 115:
                break

            try:
                title_tag = article.select_one("a.sa_text_title")
                media_tag = article.select_one("span.press")

                if not title_tag:
                    continue

                title = title_tag.text.strip()
                href = title_tag['href']
                media = media_tag.text.strip() if media_tag else "ì–¸ë¡ ì‚¬ ì—†ìŒ"

                if collection.find_one({"URL": href}):
                    print(f"â­ ì¤‘ë³µëœ ê¸°ì‚¬: {title}")
                    continue

                # ê¸°ì‚¬ ë³¸ë¬¸ ìš”ì²­ (requests)
                try:
                    news_res = requests.get(href, headers=headers, timeout=10)
                    news_res.raise_for_status()
                except requests.RequestException as e:
                    print(f"ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨ ({href}): {e}")
                    continue

                news_soup = BeautifulSoup(news_res.text, "html.parser")

                # ë³¸ë¬¸
                body_tag = news_soup.select_one("#dic_area")
                body = body_tag.text.strip() if body_tag else "ë³¸ë¬¸ ì—†ìŒ"

                # ê¸°ì
                reporter_tag = news_soup.select_one(".byline")
                reporter = reporter_tag.text.strip() if reporter_tag else "ê¸°ì ì—†ìŒ"

                # ë‚ ì§œ (ì…ë ¥ ë‚ ì§œë§Œ ì •í™•íˆ ì¶”ì¶œ)
                date = None
                for bunch in news_soup.select("div.media_end_head_info_datestamp_bunch"):
                    em_tag = bunch.find("em", class_="media_end_head_info_datestamp_term")
                    if em_tag and em_tag.text.strip() == "ì…ë ¥":
                        date_span = bunch.find("span", class_="media_end_head_info_datestamp_time")
                        if date_span and date_span.has_attr("data-date-time"):
                            date = date_span["data-date-time"]
                            break

                if not date:
                    print(f"â­ ë‚ ì§œ ì •ë³´ ì—†ìŒ: {title}")
                    continue

                # ì–¸ë¡ ì‚¬ ë¡œê³  altë¡œ ì—…ë°ì´íŠ¸
                logo_tag = news_soup.select_one("img.media_end_head_top_logo_img")
                if logo_tag and logo_tag.has_attr("alt"):
                    media = logo_tag['alt']

                # Seleniumìœ¼ë¡œ ê³µê°ìˆ˜ì™€ ëŒ“ê¸€ìˆ˜ ê°€ì ¸ì˜¤ê¸°
                driver.get(href)
                time.sleep(2)  # í˜ì´ì§€ ë Œë”ë§ ëŒ€ê¸°

                try:
                    like_elem = driver.find_element(By.CSS_SELECTOR, 'div._reactionModule span.u_likeit_text._count.num')
                    like_count = int(like_elem.text.strip().replace(',', ''))
                except Exception:
                    like_count = 0

                try:
                    comment_elem = driver.find_element(By.CSS_SELECTOR, 'a#comment_count')
                    comment_count = int(comment_elem.text.strip().replace(',', ''))
                except Exception:
                    comment_count = 0

                # ì €ì¥ ë°ì´í„° ìƒì„±
                data = {
                    "title": title,
                    "URL": href,
                    "media": media,
                    "body": body,
                    "reporter": reporter,
                    "date": date,
                    "like_count": like_count,
                    "comment_count": comment_count,
                    "crawled_at": datetime.now()
                }

                collection.insert_one(data)
                count_saved += 1
                print(f"{count_saved}. âœ… ì €ì¥ë¨: {title}")

                delay = random.uniform(4, 7)
                print(f"â³ ëŒ€ê¸° ì¤‘... {delay:.2f}ì´ˆ")
                time.sleep(delay)

            except Exception as e:
                print(f"âš ï¸ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
                continue
    finally:
        driver.quit()

    print(f"ğŸ“¦ ìµœì¢… ì €ì¥ëœ ê¸°ì‚¬ ìˆ˜: {count_saved}")


if __name__ == "__main__":
    crawl_politics_news()
